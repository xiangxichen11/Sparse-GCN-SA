@inproceedings{transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Attention is All you Need},
 year = {2017},
}

@book{ml-is-fun,
  author = {Yourgan Schmidt and Mygan HÃ¼ber},
  title = {Machine Learning is Fun},
  year = {1832},
}

@article{jiang2024self,
  title={Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding},
  author={Jiang, Mengying and Liu, Guizhong and Su, Yuanchao and Wu, Xinliang},
  journal={arXiv preprint arXiv:2403.03465},
  year={2024}
}

@article{hamilton2017representation,
  title={Representation learning on graphs: Methods and applications},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  journal={arXiv preprint arXiv:1709.05584},
  year={2017}
}

@article{fan2023markov,
  title={Markov clustering regularized multi-hop graph neural network},
  author={Fan, Xiaolong and Gong, Maoguo and Wu, Yue},
  journal={Pattern Recognition},
  volume={139},
  pages={109518},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{li2023homogcl,
  title={Homogcl: Rethinking homophily in graph contrastive learning},
  author={Li, Wen-Zhi and Wang, Chang-Dong and Xiong, Hui and Lai, Jian-Huang},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={1341--1352},
  year={2023}
}

@article{pei2020geom,
  title={Geom-gcn: Geometric graph convolutional networks},
  author={Pei, Hongbin and Wei, Bingzhe and Chang, Kevin Chen-Chuan and Lei, Yu and Yang, Bo},
  journal={arXiv preprint arXiv:2002.05287},
  year={2020}
}

@article{zhu2020beyond,
  title={Beyond homophily in graph neural networks: Current limitations and effective designs},
  author={Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and Koutra, Danai},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7793--7804},
  year={2020}
}

@article{lim2021large,
  title={Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods},
  author={Lim, Derek and Hohne, Felix and Li, Xiuyu and Huang, Sijia Linda and Gupta, Vaishnavi and Bhalerao, Omkar and Lim, Ser Nam},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20887--20902},
  year={2021}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{ye2021sparse,
  title={Sparse graph attention networks},
  author={Ye, Yang and Ji, Shihao},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={1},
  pages={905--916},
  year={2021},
  publisher={IEEE}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{dwivedi2022long,
  title={Long range graph benchmark},
  author={Dwivedi, Vijay Prakash and Ramp{\'a}{\v{s}}ek, Ladislav and Galkin, Michael and Parviz, Ali and Wolf, Guy and Luu, Anh Tuan and Beaini, Dominique},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22326--22340},
  year={2022}
}

@inproceedings{shirzad2023exphormer,
  title={Exphormer: Sparse transformers for graphs},
  author={Shirzad, Hamed and Velingker, Ameya and Venkatachalam, Balaji and Sutherland, Danica J and Sinop, Ali Kemal},
  booktitle={International Conference on Machine Learning},
  pages={31613--31632},
  year={2023},
  organization={PMLR}
}

@article{rampavsek2022recipe,
  title={Recipe for a general, powerful, scalable graph transformer},
  author={Ramp{\'a}{\v{s}}ek, Ladislav and Galkin, Michael and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14501--14515},
  year={2022}
}

@inproceedings{kong2023goat,
  title={GOAT: A global transformer on large-scale graphs},
  author={Kong, Kezhi and Chen, Jiuhai and Kirchenbauer, John and Ni, Renkun and Bruss, C Bayan and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17375--17390},
  year={2023},
  organization={PMLR}
}

@article{zhu2020beyond,
  title={Beyond homophily in graph neural networks: Current limitations and effective designs},
  author={Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and Koutra, Danai},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7793--7804},
  year={2020}
}

